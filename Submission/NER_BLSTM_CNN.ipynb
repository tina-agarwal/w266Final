{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load packages\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from Cal_F1 import compute_f1\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
    "    Flatten, concatenate\n",
    "from ner_blstm import readfile, addCharInfo, addpadding, createDataset, batchGenerator,getCasing,createEqualBatches,iterate_minibatches\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD, Nadam,Adam,Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class initialised.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialise class\"\"\"\n",
    "\n",
    "class CNN_BLSTM(object):\n",
    "    \n",
    "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
    "        \n",
    "        self.epochs = EPOCHS\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
    "        self.lstm_state_size = LSTM_STATE_SIZE\n",
    "        self.conv_size = CONV_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.optimizer = OPTIMIZER\n",
    "        \n",
    "    def loadData(self):\n",
    "        \"\"\"Load data and add character information\"\"\"\n",
    "        self.trainSentences = readfile(\"data/train.txt\")\n",
    "        self.devSentences = readfile(\"data/dev.txt\")\n",
    "        self.testSentences = readfile(\"data/test.txt\")\n",
    "\n",
    "    def addCharInformation(self):\n",
    "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
    "        self.trainSentences = addCharInfo(self.trainSentences)\n",
    "        self.devSentences = addCharInfo(self.devSentences)\n",
    "        self.testSentences = addCharInfo(self.testSentences)\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
    "\n",
    "        labelSet = set()\n",
    "        words = {}\n",
    "\n",
    "        # unique words and labels in data  \n",
    "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
    "            for sentence in dataset:\n",
    "                for token, char, label in sentence:\n",
    "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
    "                    labelSet.add(label)\n",
    "                    words[token.lower()] = True\n",
    "\n",
    "        # mapping for labels\n",
    "        self.labelIdx = {}\n",
    "        for label in labelSet:\n",
    "            self.labelIdx[label] = len(self.labelIdx)\n",
    "\n",
    "        # read GLoVE word embeddings\n",
    "        wordIdx = {}\n",
    "        self.wordEmbeddings = []\n",
    "\n",
    "        fEmbeddings = open(\"embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "\n",
    "        # loop through each word in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(wordIdx) == 0:  # add padding+unknown\n",
    "                wordIdx[\"PADDING_TOKEN\"] = len(wordIdx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                wordIdx[\"UNKNOWN_TOKEN\"] = len(wordIdx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                wordIdx[split[0]] = len(wordIdx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "\n",
    "        # dictionary of all possible characters\n",
    "        self.charIdx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
    "            self.charIdx[c] = len(self.charIdx)\n",
    "\n",
    "        # format: [[wordindices], [padded word indices], [label indices]]\n",
    "        self.train_set = addpadding(createDataset(self.trainSentences, wordIdx, self.labelIdx, self.charIdx))\n",
    "        self.dev_set = addpadding(createDataset(self.devSentences, wordIdx, self.labelIdx, self.charIdx))\n",
    "        self.test_set = addpadding(createDataset(self.testSentences, wordIdx, self.labelIdx, self.charIdx))\n",
    "\n",
    "        self.idxLabel = {v: k for k, v in self.labelIdx.items()}\n",
    "        \n",
    "    def generateBatches(self):\n",
    "        \"\"\"Generate batches\"\"\"\n",
    "        self.train_batch, self.train_batch_len = batchGenerator(self.train_set)\n",
    "        self.dev_batch, self.dev_batch_len = batchGenerator(self.dev_set)\n",
    "        self.test_batch, self.test_batch_len = batchGenerator(self.test_set)\n",
    "        \n",
    "    def tag_data(self, dataset, model):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            char = np.asarray([char])\n",
    "            pred = model.predict([tokens, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            correctLabels.append(labels)\n",
    "            predLabels.append(pred)\n",
    "        return predLabels, correctLabels\n",
    "    \n",
    "    def buildModel(self):\n",
    "        \"\"\"Model layers\"\"\"\n",
    "\n",
    "        # character input\n",
    "        character_input = Input(shape=(None, 52,), name=\"Character_input\")\n",
    "        embed_char_out = TimeDistributed(\n",
    "            Embedding(len(self.charIdx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
    "            character_input)\n",
    "\n",
    "        dropout = Dropout(self.dropout)(embed_char_out)\n",
    "\n",
    "        # CNN\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
    "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
    "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
    "        char = Dropout(self.dropout)(char)\n",
    "\n",
    "        # word-level input\n",
    "        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
    "                          trainable=False)(words_input)\n",
    "\n",
    "        # concat & BLSTM\n",
    "        output = concatenate([words, char])\n",
    "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
    "                                    return_sequences=True, \n",
    "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
    "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
    "                                   ), name=\"BLSTM\")(output)\n",
    "        output = TimeDistributed(Dense(len(self.labelIdx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
    "\n",
    "        # set up model\n",
    "        self.model = Model(inputs=[words_input, character_input], outputs=[output])\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        self.init_weights = self.model.get_weights()\n",
    "        \n",
    "        plot_model(self.model, to_file='model.png')\n",
    "        \n",
    "        print(\"Model built. Saved model.png\\n\")\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Default training\"\"\"\n",
    "\n",
    "        self.f1_test_history = []\n",
    "        self.f1_dev_history = []\n",
    "\n",
    "        for epoch in range(self.epochs):    \n",
    "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
    "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
    "                labels, tokens, char = batch       \n",
    "                self.model.train_on_batch([tokens, char], labels)\n",
    "\n",
    "            # compute F1 scores\n",
    "            predLabels, correctLabels = self.tag_data(self.test_batch, self.model)\n",
    "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idxLabel)\n",
    "            self.f1_test_history.append(f1_test)\n",
    "            print(\"f1 test \", round(f1_test, 4))\n",
    "\n",
    "            predLabels, correctLabels = self.tag_data(self.dev_batch, self.model)\n",
    "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idxLabel)\n",
    "            self.f1_dev_history.append(f1_dev)\n",
    "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
    "            \n",
    "        print(\"Final F1 test score: \", f1_test)\n",
    "            \n",
    "        print(\"Training finished.\")\n",
    "            \n",
    "        # save model\n",
    "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
    "                                                        self.dropout, \n",
    "                                                        self.dropout_recurrent, \n",
    "                                                        self.lstm_state_size,\n",
    "                                                        self.conv_size,\n",
    "                                                        self.learning_rate,\n",
    "                                                        self.optimizer.__class__.__name__\n",
    "                                                       )\n",
    "        \n",
    "        modelName = self.modelName + \".h5\"\n",
    "        self.model.save(modelName)\n",
    "        print(\"Model weights saved.\")\n",
    "        \n",
    "        self.model.set_weights(self.init_weights)  # clear model\n",
    "        print(\"Model weights cleared.\")\n",
    "\n",
    "    def writeToFile(self):\n",
    "        \"\"\"Write output to file\"\"\"\n",
    "\n",
    "        # .txt file format\n",
    "        # [epoch  ]\n",
    "        # [f1_test]\n",
    "        # [f1_dev ]\n",
    "        \n",
    "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
    "\n",
    "        fileName = self.modelName + \".txt\"\n",
    "        with open(fileName,'wb') as f:\n",
    "            for line in output:\n",
    "                np.savetxt(f, line, fmt='%.5f')\n",
    "                \n",
    "        print(\"Model performance written to file.\")\n",
    "\n",
    "    print(\"Class initialised.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set parameters\"\"\"\n",
    "\n",
    "EPOCHS = 1               # paper: 80\n",
    "DROPOUT = 0.5             # paper: 0.68\n",
    "DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
    "LSTM_STATE_SIZE = 200     # paper: 275\n",
    "CONV_SIZE = 3             # paper: 3\n",
    "LEARNING_RATE = 0.010    # paper 0.0105\n",
    "OPTIMIZER = Adam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model built. Saved model.png\n",
      "\n",
      "Epoch 0/1\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "f1 test  0.4863\n",
      "f1 dev  0.4601 \n",
      "\n",
      "Final F1 test score:  0.4863253856942496\n",
      "Training finished.\n",
      "Model weights saved.\n",
      "Model weights cleared.\n",
      "Model performance written to file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Construct and run model\"\"\"\n",
    "\n",
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "cnn_blstm.loadData()\n",
    "cnn_blstm.addCharInformation()\n",
    "cnn_blstm.create_embeddings()\n",
    "cnn_blstm.generateBatches()\n",
    "cnn_blstm.buildModel()\n",
    "cnn_blstm.train()\n",
    "cnn_blstm.writeToFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
    "plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-m: 3.17%\n",
      "I-m: 1.72%\n",
      "B-do: 1.57%\n",
      "I-do: 1.41%\n",
      "B-mo: 1.17%\n",
      "I-mo: 0.04%\n",
      "B-f: 1.42%\n",
      "I-f: 0.42%\n",
      "B-du: 0.2%\n",
      "I-du: 0.39%\n",
      "B-r: 0.58%\n",
      "I-r: 0.39%\n",
      "O: 87.52%\n"
     ]
    }
   ],
   "source": [
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "cnn_blstm.loadData()\n",
    "\n",
    "category_count = {\"B-m\\n\": 0, \"I-m\\n\":0, \"B-do\\n\": 0, \"I-do\\n\":0, \"B-mo\\n\": 0, \"I-mo\\n\": 0, \"B-f\\n\": 0, \"I-f\\n\": 0, \"B-du\\n\": 0, \n",
    "                  \"I-du\\n\": 0, \"B-r\\n\": 0, \"I-r\\n\": 0,\"O\\n\": 0}\n",
    "total_count = 0\n",
    "\n",
    "for sentence in cnn_blstm.trainSentences:\n",
    "    for word in sentence:\n",
    "        if word[1] in category_count.keys():\n",
    "            category_count[word[1]] += 1\n",
    "            total_count += 1\n",
    "\n",
    "for category, count in category_count.items():\n",
    "    print(\"{}: {}%\".format(category.replace(\"\\n\", \"\"), round((count/total_count)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
